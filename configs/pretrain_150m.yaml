# CodeForge 150M Pretraining Config — Reclaimer Protocol
# Born-quantized architecture with TCFP FP8 training
# Optimized for single consumer GPU (8-24 GB VRAM)

model: configs/model_150m.yaml
tokenizer_path: tokenizer/codeforge.json

data:
  hf_dataset: "bigcode/starcoderdata"
  languages:
    - python
    - javascript
    - typescript
    - java
    - cpp
    - c
    - go
    - rust
  language_weights:
    python: 0.35
    javascript: 0.12
    typescript: 0.12
    java: 0.10
    cpp: 0.08
    c: 0.05
    go: 0.07
    rust: 0.07
  quality_threshold: 0.35
  enable_dedup: true
  dedup_threshold: 0.8
  enable_decontamination: true
  fim_rate: 0.5
  gradient_checkpointing: true
  seed: 42

training:
  # Effective batch = 8 * 32 = 256 sequences
  # Tokens/step = 256 * 2048 = 524,288
  # 45K steps = ~23.6B tokens (~208x params, aggressive Phi-style overtraining)
  batch_size: 8
  gradient_accumulation_steps: 32
  num_workers: 2
  learning_rate: 3.0e-4
  weight_decay: 0.1
  max_grad_norm: 1.0
  min_lr_ratio: 0.1
  max_steps: 45000
  precision: bf16

  # WSD schedule: 1.5K warmup + 36K stable + 7.5K decay = 45K steps
  scheduler_type: wsd
  warmup_steps: 1500
  stable_steps: 36000
  decay_steps: 7500
  decay_type: cosine
  checkpoint_dir: checkpoints/pretrain_150m
  checkpoint_every: 500
  eval_every: 2500
  log_every: 10
  use_wandb: false
  wandb_project: codeforge-150m

  # Reclaimer Protocol: separate embedding LR
  embed_lr_ratio: 0.1

  # TCFP — Tensor Core Floating Point (custom FP8 format)
  use_tcfp: true
  tcfp_warmup_steps: 0          # FP8 from step 0 (born-quantized)
  tcfp_use_tensor_cores: true   # Real FP8 GEMMs via torch._scaled_mm
  tcfp_delayed_scaling: true    # EMA-smoothed W_hi amax for numerical stability
  tcfp_use_fused_kernel: true   # Triton fused dual-GEMM (load activation once)
  tcfp_use_cuda_ext: true       # cuBLASLt C++ extension when available
  tcfp_abd: false               # Conservative: keep both GEMMs in backward
  tcfp_srr: false               # Quality-first: use EF + delayed_scaling
  tcfp_monitor_interval: 100    # Log EF buffer + kurtosis health every 100 steps
  tcfp_disable_compile: true    # Triton kernels already JIT; compile adds overhead
